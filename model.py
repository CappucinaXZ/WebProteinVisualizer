# -*- coding: utf-8 -*-
"""Untitled22.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qIJHN3bYZ0MlWTqQt7oWtTcEnh3CUZ3o
"""

import random
import os
import torch
from torch import nn, einsum
from torch.autograd import Variable

import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import gc

from einops import rearrange, repeat, reduce
from einops.layers.torch import Rearrange
from inspect import isfunction

import sidechainnet as scn
from sidechainnet.examples import losses, models
from sidechainnet.structure.structure import inverse_trig_transform
from sidechainnet.structure.build_info import NUM_ANGLES
import py3Dmol

import math

seed = 0

random.seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)

# helpers

def exists(val):
    return val is not None

def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d

def cast_tuple(val, depth = 1):
    return val if isinstance(val, tuple) else (val,) * depth

def init_zero_(layer):
    nn.init.constant_(layer.weight, 0.)
    if exists(layer.bias):
        nn.init.constant_(layer.bias, 0.)

class Attention(nn.Module):
    def __init__(
        self,
        dim,
        seq_len = None,
        heads = 8,
        dim_head = 64,
        dropout = 0.0,
        gating = True
    ):
        super().__init__()
        inner_dim = dim_head * heads
        self.seq_len = seq_len
        self.heads= heads
        self.scale = dim_head ** -0.5

        self.to_q = nn.Linear(dim, inner_dim, bias = False)
        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)
        self.to_out = nn.Linear(inner_dim, dim)

        self.gating = nn.Linear(dim, inner_dim)
        nn.init.constant_(self.gating.weight, 0.)
        nn.init.constant_(self.gating.bias, 1.)

        self.dropout = nn.Dropout(dropout)
        init_zero_(self.to_out)

    def forward(self, x, mask = None, attn_bias = None, context = None, context_mask = None, tie_dim = None):
        device, orig_shape, h, has_context = x.device, x.shape, self.heads, exists(context)
        context = default(context, x)
        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))
        i, j = q.shape[-2], k.shape[-2]
        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), (q, k, v))

        # scale
        q = q * self.scale

        # query / key similarities
        if exists(tie_dim):
            # as in the paper, for the extra MSAs
            # they average the queries along the rows of the MSAs
            # they named this particular module MSAColumnGlobalAttention

            q, k = map(lambda t: rearrange(t, '(b r) ... -> b r ...', r = tie_dim), (q, k))
            q = q.mean(dim = 1)

            dots = einsum('b h i d, b r h j d -> b r h i j', q, k)
            dots = rearrange(dots, 'b r ... -> (b r) ...')
        else:
            dots = einsum('b h i d, b h j d -> b h i j', q, k)

        # add attention bias, if supplied (for pairwise to msa attention communication)
        if exists(attn_bias):
            dots = dots + attn_bias

        # masking
        if exists(mask):
            mask = default(mask, lambda: torch.ones(1, i, device = device).bool())
            context_mask = mask if not has_context else default(context_mask, lambda: torch.ones(1, k.shape[-2], device = device).bool())
            mask_value = -torch.finfo(dots.dtype).max
            mask = mask[:, None, :, None] * context_mask[:, None, None, :]
            try:
                mask = mask.to(torch.bool)
                dots = dots.masked_fill(~mask, mask_value)
            except:
                dots = dots.masked_fill(mask, mask_value)

        # attention
        dots = dots - dots.max(dim = -1, keepdims = True).values
        attn = dots.softmax(dim = -1)
        attn = self.dropout(attn)
        # aggregate
        out = einsum('b h i j, b h j d -> b h i d', attn, v)
        # merge heads
        out = rearrange(out, 'b h n d -> b n (h d)')
        # gating
        gates = self.gating(x)
        out = out * gates.sigmoid()
        # combine to out
        out = self.to_out(out)
        return out

class PositionalEncoder(nn.Module):
    def __init__(self, d_model, max_seq_len = 4000):
        super().__init__()
        self.d_model = d_model
        
        # create constant 'pe' matrix with values dependant on 
        # pos and i
        pe = torch.zeros(max_seq_len, d_model)
        for pos in range(max_seq_len):
            for i in range(0, d_model, 2):
                pe[pos, i] = \
                math.sin(pos / (10000 ** ((2 * i)/d_model)))
                pe[pos, i + 1] = \
                math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))
                
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
 
    
    def forward(self, x):
        # make embeddings relatively larger
        x = x * math.sqrt(self.d_model)
        #add constant to embedding
        seq_len = x.size(1)
        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)
        return x

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=1024, dropout = 0.1):
        super().__init__() 
        # We set d_ff as a default to 2048
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)
        self.gelu = nn.GELU()
    def forward(self, x):
        x = self.dropout(self.gelu(self.linear_1(x)))
        x = self.linear_2(x)
        return x

class ProteinNet(nn.Module):

    def __init__(self,
                 d_hidden,
                 dim,
                 d_in=21,
                 d_embedding=32,
                 heads = 8,
                 integer_sequence=True,
                 dropout = 0.1,
                 n_angles=scn.structure.build_info.NUM_ANGLES):
        
        super(ProteinNet, self).__init__()

        self.d_hidden = d_hidden
        self.d_out = n_angles * 2

        self.integer_sequence = integer_sequence
        if self.integer_sequence:
            self.input_embedding = torch.nn.Embedding(d_in, dim, padding_idx=20)
        else:
            self.input_embedding = torch.nn.Linear(d_in, dim)

        self.pe = PositionalEncoder(dim)

        self.norm = nn.LayerNorm([dim])
        self.norm_1 = nn.LayerNorm([dim])
        self.norm_2 = nn.LayerNorm([dim])

        self.attn = Attention(dim = dim, heads = heads)
        self.ff = FeedForward(dim)

        self.dropout_1 = nn.Dropout(dropout)
        self.dropout_2 = nn.Dropout(dropout)

        self.final = nn.Sequential(nn.GELU(),
                                   nn.Linear(dim, self.d_out))
                            
        self.output_activation = torch.nn.Tanh()

    def get_lengths(self, sequence):
        """Compute the lengths of each sequence in the batch."""
        if self.integer_sequence:
            lengths = sequence.shape[-1] - (sequence == 20).sum(axis=1)
        else:
            lengths = sequence.shape[1] - (sequence == 0).all(axis=-1).sum(axis=1)
        return lengths.cpu()

    def forward(self, sequence, mask=None):

        lengths = self.get_lengths(sequence)

        sequence = self.input_embedding(sequence)
        # output = self.pe(sequence)

        output = self.norm_1(sequence)
        output = self.attn(output, mask=mask)
        output = self.dropout_1(output)
        output = self.norm_2(output)
        output = self.ff(output)
        output = self.dropout_2(output)
        output = self.norm(output)
        output = self.final(output)

        output = self.output_activation(output)

        output = output.view(output.shape[0], output.shape[1], 12, 2)

        return output

pssms_model = ProteinNet(d_hidden=512,
                           dim=256,
                           d_in=49,
                           d_embedding=32,
                           integer_sequence=True)

path = "model_final_D_2023_04_26-02_57_32_PM.pth"
checkpoint = torch.load(path)
pssms_model.load_state_dict(checkpoint['model_state_dict'])

def seq2onenot(seq):
  
  alphabet = ['A', 'C', 'D', 'E', 'F', 'G','H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']
  char_to_int = dict((c, i) for i, c in enumerate(alphabet))
  integer_encoded = [char_to_int[char] for char in data]
  onehot_encoded = list()
  for value in integer_encoded:
    letter = [0 for _ in range(len(alphabet))]
    letter[value] = 1
    onehot_encoded.append(letter)
  onehot = np.array(onehot_encoded)

  return onehot

def seq2pdb(onehot):

  predicted_angles_sincos = pssms_model(onehot)
  predicted_angles = inverse_trig_transform(predicted_angles_sincos)
  sb = scn.StructureBuilder(onehot, predicted_angles)
  sb.to_pdb("output.pdb")